{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118B - Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "Hopefully your team is at least this good. Obviously you should replace these with your names.\n",
    "\n",
    "- Jensen McKenzie\n",
    "- Samil Ahsan\n",
    "- Junghwan Kim\n",
    "- Abishek Siva\n",
    "- Aditya Agrawal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "\n",
    "In this study, our objective is to categorize stocks based on their quantified return and volatility. By employing clustering techniques, we aim to investigate whether return and volatility serve as accurate indicators for grouping stocks in a meaningful manner. To achieve this, we utilize extensive datasets obtained from yfinance and Morningstar, encompassing a vast array of ticker symbols for each stock. Employing variables such as price, volume, and performance metrics, we normalize the data and apply machine learning methodologies, including K-means clustering and hierarchical clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "The financial markets are complex ecosystems influenced by a myriad of factors, including economic indicators, geopolitical events, investor sentiment, and company-specific information. Analyzing and understanding the intricate relationships within this dynamic environment is crucial for making informed investment decisions. Traditional approaches to stock analysis often fall short in capturing the nuanced patterns and interactions that drive market movements.\n",
    "\n",
    "In recent years, machine learning and data science techniques have gained prominence in financial analysis, offering a more sophisticated and data-driven perspective. One such powerful technique is stock clustering, a method that groups similar stocks based on historical price movements, trading volumes, and other relevant features. The underlying assumption is that stocks with similar characteristics are likely to respond similarly to market dynamics.\n",
    "\n",
    "There has been a lot of relevant work conducted regarding this topic, with interesting results. In a study done in Argentinia about using clusetring technquies to enhance stock returns forecasting, they used K-means and for each cluster, used ARIMA(Autoregressive Integrated Moving Average) and LSTM(Long Short-Term Memory) forecasting models and test their performances. The study showed that there was enhanced forecasting precision by leveraging the additional information offered by clustering methods, underscoring the significance of relevant data selection in preprocessing. Moreover, using the whole sample of stocks only worsened the forecasting ability of LSTM model<a name=\"fn1\"></a>[<sup>[1]</sup>](#fn1note).\n",
    "\n",
    "In another paper, the researchers used clustering-enhanced deep learning framework to predict the stock prices using LSTM, RNN (Recurrent Neural Network), and GRU(Gated Recurrent Unit) models. To enhance clustering effectiveness in the context of stock price time series, this study introduces a novel similarity measure known as Logistic Weighted Dynamic Time Warping (LWDTW) for calculating distances between stock price data points. In comparison with benchmark measures, such as Euclidean distance and standard Dynamic Time Warping (DTW), LWDTW incorporates a weight function that acknowledges the non-normal distribution of stock returns. Empirical analysis of individual US stock price data reveals characteristics like dynamic, non-stationary, nonlinear, and chaotic behaviors, better represented by a logistic distribution probability density function with higher peaks and fatter tails. LWDTW leverages this insight by using the logistic distribution as the weight function, assigning appropriate weights to extreme return observations while emphasizing normal return observations in distance matrix calculations. This approach ensures that the clustering method accounts for the unique patterns in stock returns<a name=\"fn2\"></a>[<sup>[2]</sup>](#fn2note).\n",
    "\n",
    "Although a lot of researches focus on predicitng the stock prices like above, they all gave very helpful insights on how to effectively utilize clustering methods for much accurate prediciting and forecasting aftwerwards. Both studies mentioned above displayed the importance of proper clustering methods, giving us tips and even prompted to create a new clustering methods. Overall, understandings of these studies could gave us better idea of clustering and make us more aware of things to consider when implementing clustering on stocks and utilize their knowledge into our own project. \n",
    "\n",
    "<sup id=\"fn1\">1. [Javier Vásquez Sáenz, Facundo Manuel Quiroga, Aurelio F. Bariviera,Data vs. information: Using clustering techniques to enhance stock returns forecasting, International Review of Financial Analysis, Volume 88, 2023, 102657, ISSN 1057-5219, https://doi.org/10.1016/j.irfa.2023.102657.]<a href=\"#ref1\" title=\"Jump back to footnote 1 in the text.\">↩</a></sup>\n",
    "\n",
    "<sup id=\"fn2\">2. [Li, M., Zhu, Y., Shen, Y. et al. Clustering-enhanced stock price prediction using deep learning. World Wide Web 26, 207–232 (2023). https://doi.org/10.1007/s11280-021-01003-0]<a href=\"#ref2\" title=\"Jump back to footnote 2 in the text.\">↩</a></sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "The problem we are aiming to solve involves creating a machine learning model that can automate the clustering of stocks based on their return and volatility. This involves two very broad steps:\n",
    "\n",
    "1. Quantify the return and volatility of our targeted stock data observations over a certain period of time.\n",
    "\n",
    "2. Cluster stocks into distinct groups with very similar return and volatility tendencies or behaviors using these quantified variables.\n",
    "\n",
    "### ML-Relevant Potential Solution\n",
    "As a result of looking into the clustering of different stocks, this project naturally has machine learning relevant solutions that involve clustering techniques such as K-means or hierarchical clustering. Using clustering techniques also lends itself to expanding with more machine learning methods, including but not limited to PCA in order to reduce dimensionality or outlier removal. Feeding the model historical data about the stocks' properties should allow it to learn to identify patterns and group stocks with similar characteristics. This automated clustering can help investors make more informed decisions identifying groups of stocks that align more with their investment goals.\n",
    "\n",
    "### Quantifiability\n",
    "The return will be determined by the percentage of change in stock price over our set period of time, whereas, volatility can be quantified using the standard deviation of the stock's returns over that same period of time. Due to the nature of the variables we are measuring, this project is highly quantifiable as we are using historical stock price data and performing mathematical compuations on the data.\n",
    "\n",
    "### Measurability and Observability\n",
    "We will ensure measurability by using the variety of measured ticker symbols present in finance and stock datasets that contain significant amounts of historical data leading up to current times. Not only do we aim to have variables that are easily measured and observed, but we also strive to measure the success of the machine learning model and its ability to accurately cluster stocks in a meaningful way. We can use certain metrics such as the Davies-Bouldin index to evaluate the quality of clusters by measuring similarity of stocks to other stocks in the same cluster.\n",
    "\n",
    "### Replicability\n",
    "Clustering stocks based on return, volatility, and other metrics that are contained in most financial datasets will guarantee replicability in the sense that the model can be used across a multitude of time periods. Using a machine learning model trained on historical data will allow us to continuously feed data as it arrives over time to create clusters that represent the latest market trends. This ensures that the clustering will remain relevant and can still be used to inform investment decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "This project will use financial data provided by the python packages [yfinance](https://pypi.org/project/yfinance/) and [morningstar-data](https://pypi.org/project/morningstar-data/). Both of these datasets contain data for almost every ticker symbol availible on the stock market. This includes current price, past price, various performance metrics, and other financial data.\n",
    "\n",
    "We plan to primarily use yfinance for the majority of the training data, and use morningstar-data to validate our model, using helpful proprietary metrics curated by financial experts. An observation for a single stock ticker would consist of one row of data, with each column representing a different financial metric. The critical variables would be the price of the stock, the volume of the stock, and the various performance metrics.\n",
    "\n",
    "One possible issue with this data is lack of normalization. For instance, we could have a stock trading at $1000, and another trading at $10. This would make it difficult to compare the two stocks, as the price of the stock would be heavily weighted in the model. We would need to normalize the data to ensure that the model is not biased towards stocks with higher prices. Additionally, we would most likely want to consider market cap as a metric, because this is a key indicator of risk and volatility, and can be used to normalize the data. We may need to transform any information about a stock's market sector using one-hot encoding, as this could be a key indicator of the stock's performance, and a possible evaluation metric. Because of this, we will look into metrics that place each ticker in a relatively short list of categories, such as the GICS sector classification, which is a widely used classification system for stocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "The solution to the problem described previously is to use clustering algorithms to generate a model capable of categorizing different stocks based on certain financial variables, including stock return and volatility. Our goal is to process historical stock price data using the datasets mentioned earlier to calculate both return and volatility as well as other metrics that may prove useful, eventually applying clustering techniques to group different stocks into clusters where each member shares similar characteristics surrounding our measured variables. Although there is no way to 100% predict the trends of stock prices and there are many studies bolstering the ideas behind the random walk theory (changes in asset prices are random), our goal is to create a model that can inform investment decisions by exhibiting trends rather than guarantees.\n",
    "\n",
    "### Algorithmic Description\n",
    "1. **Data Extraction**: Initially, we will have to collect relevant data from public datasets that contain enough information to generate a model that accurately clusters stocks. In essence, this step will mainly consist of importing the appropriate datasets, cleaning the respective data to handle missing values and various data formats, normalizing the data to an appropriate scale, and finally calculating our desired metrics.\n",
    "\n",
    "2. **Clustering**: In order to cluster the data accurately, we will utilize the K-means clustering algorithm, which is fully capable of efficiently and effectively clustering data based on similarity in features. In the context of stocks, the clustering algorithm will randomly initialize k centroids and iteratively assigning stocks that have similar metric values (return and volatility) to the nearest centroid. The algorithm will then update centroids based on the current clustering until the data converges into reasonable groups of stocks. K-means is a simple and efficient algorithm that can handle large datasets, making it perfectly suited for the problem that we have set to solve as the datasets we will be using have a large amount of stock observations and metrics. An additional bonus to using this algorithm is that we can assign the predetermined k value of clusters before performing the actual clustering, resulting in flexibility in the categorization as we can compare the models when using different amounts of clusters.\n",
    "\n",
    "3. **Implementation**: Although we have not yet confirmed which libraries we will be using, our imported packages will most likely include the financial dataset packages mentioned earlier, \"pandas\" for data manipulation and cleaning, \"numpy\" for efficient computation involving the data, and finally \"scikit-learn\" since it has all of the relevant tools and functions for implementing a K-means clustering algorithm.\n",
    "\n",
    "4. **Testing**: The solution can be tested in a plethora of ways, however, we will specifically be using the silhouette score and Davies-Bouldin index in an attempt to internally evaluate the clustering effectiveness. These metrics can be used to measure how similar each datapoint is to its own cluster as opposed to other clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "The silhouette coefficient is an evaluation metric for assessing the quality of clustering results. It measures how well-separated clusters are and how similar data points are to their own cluster compared to other clusters.\n",
    "\n",
    "The silhouette coefficient for a single sample is given as:\n",
    "\n",
    "$s = \\frac{b-a}{max(a, b)}$\n",
    "\n",
    "Where $a$ is the mean distance between a sample and all other points in the same class and $b$ is the mean distance between a sample and all other points in the next nearest cluster.\n",
    "\n",
    "The silhouette coefficient ranges from -1 to 1:\n",
    "\n",
    "* A score close to 1 indicates that the data point is well-clustered, with a large separation from other clusters.\n",
    "* A score around 0 indicates that the data point is close to the decision boundary between two clusters.\n",
    "* A score less than 0 indicates that the data point may have been assigned to the wrong cluster.\n",
    "\n",
    "The overall silhouette score for the entire clustering is the average of the silhouette coefficients for all data points. A higher silhouette score indicates better clustering quality.\n",
    "\n",
    "The silhouette coefficient is intuitive, easy to interpret, and does not require knowledge of ground truth labels, making it suitable for evaluating clustering algorithms in unsupervised settings. It can be used to compare the performance of different clustering algorithms or to determine the optimal number of clusters for a given dataset. Therefore, it can be an appropriate evaluation metric for both the benchmark model and the solution model in your context of clustering stocks based on financial variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are dealing with financial data, there are certainly ethical and privacy issues that must be considered:\n",
    "* **Data Privacy**: The use of financial data, especially if it contains personally identifiable information or sensitive financial information, raises concerns about data privacy. Ensuring that data is anonymized in order to protect the privacy of individuals is essential.\n",
    "* **Transparency**: The decisions made by financial models can have significant impacts on individuals and markets. Therefore, it's important to ensure that the models are transparent and interpretable, allowing stakeholders to understand how decisions are made and identify any potential errors or biases.\n",
    "* **Potential for Financial Harm**: Incorrect or biased recommendations from the model could lead to financial losses for investors. It's essential to rigorously test the model and consider potential risks before deploying it in real-world investment decisions. We should also ensure that users of the model are aware of the potenial for financial loss before making any decisions based on the output of the model.\n",
    "* **Regulatory Compliance**: Financial markets are highly regulated, and any tool or model used for investment decisions must comply with relevant regulations, such as those related to market manipulation, insider trading, and investor protection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Put things here that cement how you will interact/communicate as a team, how you will handle conflict and difficulty, how you will handle making decisions and setting goals/schedule, how much work you expect from each other, how you will handle deadlines, etc... -->\n",
    "\n",
    "* Meet Mondays at 5:30pm to work on/assign parts of final project\n",
    "* Try to communicate punctually about work/availability/any changes (< 24 hours)\n",
    "* Work equally on assigned parts of the project and start early\n",
    "* Be respectfuly, communicative, and engaged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Replace this with something meaningful that is appropriate for your needs. It doesn't have to be something that fits this format.  It doesn't have to be set in stone... \"no battle plan survives contact with the enemy\". But you need a battle plan nonetheless, and you need to keep it updated so you understand what you are trying to accomplish, who's responsible for what, and what the expected due dates are for each item. -->\n",
    "\n",
    "| Meeting Date | Meeting Time    | Completed Before Meeting                                                                                                                                                   | Discuss at Meeting                                                                                                             |\n",
    "| ------------ | --------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| 2/12         | 1 PM            | Brainstorm topics/questions (all)                                                                                                                                          | Determine best form of communication; Discuss and decide on final project topic; discuss hypothesis; begin background research |\n",
    "| 2/18         | 10 AM           | Work on Problem Statement and proposed solutions (Sam) Write up evaluation metrics and ethics/privacy (Abhishek) Team Expectations and Timelines (Adi) Abstract, BG (Will) Data Section (Jensen) | Discuss ideal dataset(s) and ethics; draft project proposal                                                                    |\n",
    "| 2/20         | 10 AM           | Edit, finalize, and submit proposal (all)                                                                                                                                  | Discuss Wrangling and possible analytical approaches; Assign group members to lead each specific part                          |\n",
    "| 2/26         | 6 PM            | Import & Wrangle Data ,do some EDA (Aditya)                                                                                                                                | Review/Edit wrangling/EDA; Discuss Analysis Plan                                                                               |\n",
    "| 3/2         | 12 PM           | Finalize wrangling/EDA; Begin programming for project (ALL)                                                                                                                | Discuss/edit project code; Complete project                                                                                    |\n",
    "| 3/13         | 12 PM           | Complete analysis; Draft results/conclusion/discussion (ALL)                                                                                                               | Discuss/edit full project                                                                                                      |\n",
    "| 3/19         | Before 11:59 PM | Perfect / Double Check                                                                                                                                                     | Turn in Final Project                                                                                                          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<sup id=\"fn1\">1. [Javier Vásquez Sáenz, Facundo Manuel Quiroga, Aurelio F. Bariviera,Data vs. information: Using clustering techniques to enhance stock returns forecasting, International Review of Financial Analysis, Volume 88, 2023, 102657, ISSN 1057-5219, https://doi.org/10.1016/j.irfa.2023.102657.]<a href=\"#ref1\" title=\"Jump back to footnote 1 in the text.\">↩</a></sup>\n",
    "\n",
    "<sup id=\"fn2\">2. [Li, M., Zhu, Y., Shen, Y. et al. Clustering-enhanced stock price prediction using deep learning. World Wide Web 26, 207–232 (2023). https://doi.org/10.1007/s11280-021-01003-0]<a href=\"#ref2\" title=\"Jump back to footnote 2 in the text.\">↩</a></sup>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
